{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "authorship_tag": "ABX9TyMOUFYah4Qi+Za5RvStVpSx",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/apd1995/AMP_matrix_recovery/blob/apd1995-JS-hyperspectral/hyperspectral.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Joint recovery of hyperspectral images"
   ],
   "metadata": {
    "id": "3K419l3b3rpf"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A8qsHSUqlM00"
   },
   "source": [
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "import pywt\n",
    "import numpy as np\n",
    "import torch"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We use the Indian pines dataset available at https://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes for illustration."
   ],
   "metadata": {
    "id": "1HDGPUf64LB8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "data = loadmat('Indian_pines.mat')\n",
    "dat = data['indian_pines']\n",
    "dat = np.transpose(dat, (2, 0, 1))"
   ],
   "metadata": {
    "id": "YOvbtrcFliZP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that the image is made up of 220 slices (each corresponding to a wavelength) and each slice is of shape 145 by 145."
   ],
   "metadata": {
    "id": "9PlhWP-n4P1a"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dat.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ITyMV7galkmn",
    "outputId": "2e13209e-3784-460a-a4ea-ae59ef63bc58"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "wave_obj = pywt.dwtn(dat, 'db2', mode = 'periodization')\n",
    "wave_obj['aaa'].shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nZnYeH_H4r1t",
    "outputId": "21f56e3e-1ca3-4358-a3c8-7607edd1bf72"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "new_dat = np.zeros_like(dat, dtype = float)\n",
    "for c in range(dat.shape[2]): # Assuming image has 3 channels (RGB)\n",
    "    min_val = np.min(dat[:, :, c])\n",
    "    max_val = np.max(dat[:, :, c])\n",
    "    new_dat[:, :, c] = (dat[:, :, c] - min_val) / (max_val - min_val)\n"
   ],
   "metadata": {
    "id": "FVDw_0_7h97b"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can look at some of the slices."
   ],
   "metadata": {
    "id": "xYwsa6mr4pSe"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "quantile_prob = 0.01 # sparsity level\n",
    "original_slice_flat = original_slice.flatten()\n",
    "cA, cD = pywt.dwt(original_slice_flat, 'db3')\n",
    "cD_jitter = cD + np.random.normal(0, 1e-6, len(cD))\n",
    "cD_max = np.abs(np.max(cD))\n",
    "cD_inv = cD_max/cD_jitter\n",
    "comparison_quantile = np.quantile(np.abs(cD_inv), quantile_prob)\n",
    "cD_inv = np.where(np.abs(cD_inv)<comparison_quantile, cD_inv, np.inf)\n",
    "cD_sparse = cD_max/cD_inv\n",
    "recovered_slice_flat = pywt.idwt(cA, cD_sparse, 'db3')\n",
    "recovered_slice_flat = recovered_slice_flat[0:(len(recovered_slice_flat)-1)]\n",
    "recovered_slice = recovered_slice_flat.reshape(145, 145)"
   ],
   "metadata": {
    "id": "uDagQ0lTE9EJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "fig = plt.figure(figsize = (12, 3))\n",
    "ax = fig.add_subplot(1, 4, 1)\n",
    "ax.set_title(\"original\", fontsize=10)\n",
    "ax.imshow(original_slice, interpolation=\"nearest\")\n",
    "ax = fig.add_subplot(1, 4, 2)\n",
    "ax.set_title(\"reconstructed\", fontsize=10)\n",
    "ax.imshow(recovered_slice, interpolation=\"nearest\")\n",
    "ax = fig.add_subplot(1, 4, 3)\n",
    "ax.set_title(\"difference\", fontsize=10)\n",
    "ax.imshow(original_slice - recovered_slice, interpolation = \"nearest\", cmap = plt.cm.gray)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "Z6onWB5K8j-8",
    "outputId": "5fccfda6-fc64-4edf-8476-e68a9ceb272e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now do a wavelet decomposition flattening each slice and plot the difference coefficients in the same plot."
   ],
   "metadata": {
    "id": "RfwEyjBS5DLb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for slice in range(20):\n",
    "  quantile_prob = 0.1 # sparsity level\n",
    "  original_slice_flat = original_slice.flatten()\n",
    "  cA, cD = pywt.dwt(original_slice_flat, 'db3')\n",
    "  cD_jitter = cD + np.random.normal(0, 1e-6, len(cD))\n",
    "  cD_max = np.abs(np.max(cD))\n",
    "  cD_inv = cD_max/cD_jitter\n",
    "  comparison_quantile = np.quantile(np.abs(cD_inv), quantile_prob)\n",
    "  cD_inv = np.where(np.abs(cD_inv)<comparison_quantile, cD_inv, np.inf)\n",
    "  cD_sparse = cD_max/cD_inv\n",
    "  recovered_slice_flat = pywt.idwt(cA, cD_sparse, 'db3')\n",
    "  recovered_slice_flat = recovered_slice_flat[0:(len(recovered_slice_flat)-1)]\n",
    "  recovered_slice = recovered_slice_flat.reshape(145, 145)"
   ],
   "metadata": {
    "id": "d2q7wxSYl4HB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compressed sensing"
   ],
   "metadata": {
    "id": "xlnP_-df0uNZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "signal_ncol = 20\n",
    "X_signal_nonsparse = np.zeros((len(cD), signal_ncol), dtype = float)\n",
    "# quantile_prob = 0.1 # sparsity level\n",
    "for slice in range(signal_ncol):\n",
    "  original_slice = dat[:, :,slice]\n",
    "  original_slice_flat = original_slice.flatten()\n",
    "  cA, cD = pywt.dwt(original_slice_flat, 'db3')\n",
    "  # cD_jitter = cD + np.random.normal(0, 1e-6, len(cD))\n",
    "  # cD_max = np.abs(np.max(cD))\n",
    "  # cD_inv = cD_max/cD_jitter\n",
    "  # comparison_quantile = np.quantile(np.abs(cD_inv), quantile_prob)\n",
    "  # cD_inv = np.where(np.abs(cD_inv)<comparison_quantile, cD_inv, np.inf)\n",
    "  # cD_sparse = cD_max/cD_inv\n",
    "  X_signal[:, slice] = cD\n",
    "\n",
    "  # recovered_slice_flat = pywt.idwt(cA, cD_sparse, 'db3')\n",
    "  # recovered_slice_flat = recovered_slice_flat[0:(len(recovered_slice_flat)-1)]\n",
    "  # recovered_slice = recovered_slice_flat.reshape(145, 145)"
   ],
   "metadata": {
    "id": "28U3DVBh6GOm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "quantile_prob = 0.1 # sparsity level\n",
    "row_norm_quantile = np.quantile(np.sum(np.abs(X_signal), axis = 1), 1 - quantile_prob)\n",
    "zero_rows = (np.sum(np.abs(X_signal), axis = 1) < row_norm_quantile)\n",
    "X_signal[zero_rows,:] = 0.0"
   ],
   "metadata": {
    "id": "y1MxeAiMXvtb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# joint sparsity\n",
    "np.mean(np.sum(X_signal**2, axis = 1) != 0)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AmC_Oatb19D9",
    "outputId": "463ea6c5-76ca-4d0d-cdac-716711ec9357"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# zero pad by artificially creating zero slices to make 256 by 256 by 256\n",
    "import numpy as np\n",
    "\n",
    "# Create a zero-filled tensor of the desired shape\n",
    "padded_dat = np.zeros((2**int(np.ceil(np.log2(dat.shape[0]))), 2**int(np.ceil(np.log2(dat.shape[1]))), 2**int(np.ceil(np.log2(dat.shape[2])))))\n",
    "\n",
    "# Calculate the starting indices to center the original tensor in the new tensor\n",
    "start_x = 0 # (256 - 145) // 2\n",
    "start_y = 0 # (256 - 145) // 2\n",
    "start_z = 0 # (256 - 220) // 2\n",
    "\n",
    "# Insert the original tensor into the zero-filled tensor\n",
    "padded_dat[start_x:start_x+dat.shape[0], start_y:start_y+dat.shape[1], start_z:start_z+dat.shape[2]] = dat\n",
    "print(padded_dat.shape)\n",
    "plt.imshow(padded_dat[190, :, :])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491
    },
    "id": "_5_x75jhR8N6",
    "outputId": "e9e87135-f49f-4fda-f212-78e55e3ac8db"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's do a 3d wavelet decomposition. This is equivalent to doing a Haar decomposition along the spectral dimension and 2d wavelet decomposition along the spatial dimensions."
   ],
   "metadata": {
    "id": "CCsZD5Iyl1JD"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dat = dat[:, :128, :128]"
   ],
   "metadata": {
    "id": "Bg1HQdBRZ0bO"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "wave_coeffs = pywt.wavedecn(dat, mode = 'periodization', wavelet = 'db2', level = 3)\n",
    "# wave_coeffs = pywt.wavedecn(padded_dat, mode = 'periodization', wavelet = 'db2', level = 3)"
   ],
   "metadata": {
    "id": "ZF9qqDR3l9_F"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(wave_coeffs[0].shape)\n",
    "print(wave_coeffs[1].keys())\n",
    "print([wave_coeffs[1][key].shape for key in wave_coeffs[1].keys()])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9wgsgBxaCzsX",
    "outputId": "355f2887-ad39-4403-e087-1922d8f5de23"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.plot(np.sort((np.abs(wave_coeffs[1]['aad'][0,:,:]).flatten())))\n",
    "plt.plot(np.sort((np.abs(wave_coeffs[1]['aad'][1,:,:]).flatten())))\n",
    "plt.plot(np.sort((np.abs(wave_coeffs[1]['aad'][2,:,:]).flatten())))\n",
    "plt.plot(np.sort((np.abs(wave_coeffs[1]['aad'][10,:,:]).flatten())))\n",
    "plt.plot(np.sort((np.abs(wave_coeffs[1]['aad'][12,:,:]).flatten())))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "pwGRx-rJuULC",
    "outputId": "ab02b5f8-9bc6-46a6-c7ca-2a3fd39fa373"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.plot(wave_coeffs[0][5,:,:].flatten())\n",
    "plt.plot(wave_coeffs[0][15,:,:].flatten())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "id": "iy3EZhaeD1d8",
    "outputId": "66202b77-ea55-4970-af1f-6f25bac833aa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.plot(wave_coeffs[1]['aad'][12,:,:].flatten())\n",
    "plt.plot(wave_coeffs[1]['aad'][5,:,:].flatten())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "msoWkuWEDhse",
    "outputId": "284c9144-1944-468a-dbe4-294d5383a419"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.plot(wave_coeffs[1]['aad'][12,:,:].flatten())\n",
    "plt.plot(wave_coeffs[1]['add'][12,:,:].flatten())\n",
    "plt.plot(wave_coeffs[1]['ddd'][12,:,:].flatten())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "Um90JAD9JN52",
    "outputId": "a03f1f6a-a5f6-4d21-ee55-e8f704e927e6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is quite interesting that after doing the 3d wavelet decomposition, we do indeed get 0's in all the slices. The following code checks this is indeed the case and counts such block-sparsity. Never mind, this was an artefact of zero-padding."
   ],
   "metadata": {
    "id": "6XQFdQPU2Nox"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_sparsity(coefs_dat):\n",
    "  count_sum = 0\n",
    "  for i in range(coefs_dat.shape[1]):\n",
    "    for j in range(coefs_dat.shape[2]):\n",
    "      if np.sum(coefs_dat[i, j, :]**2) == 0.:\n",
    "        count_sum = count_sum + 1\n",
    "\n",
    "  return count_sum/(coefs_dat.shape[1]*coefs_dat.shape[2])"
   ],
   "metadata": {
    "id": "-nNeLIkS2JWa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "coefs_now = wave_coeffs[1]['aad']\n",
    "get_sparsity(coefs_now)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yJ7ueqJoDWQX",
    "outputId": "a12ba558-71c3-49c4-ffc7-47a781ac104c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Wow! We have about 41% joint sparsity! Let's try for the other difference coefficients."
   ],
   "metadata": {
    "id": "PcG_X4Pg3gvN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "for key in list(wave_coeffs[1].keys()):\n",
    "  coefs_now = wave_coeffs[1][key]\n",
    "  joint_sparsity_level = get_sparsity(coefs_now)\n",
    "  print('joint sparsity in ' + str(key) + ' is ' + str(joint_sparsity_level))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A5zP5MxG31w6",
    "outputId": "08b92857-28dd-4e3c-9300-d8450f4be0f4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We treat the difference coefficients as sparse mean plus noise and the nose level needs to be estimated. We choose MAD to ensure robustness."
   ],
   "metadata": {
    "id": "Lm-mEDxW4xnH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "plt.imshow(wave_coeffs[0][, :, :])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "dP8hwyfyZCd_",
    "outputId": "6184e152-21f1-4151-aaa5-e9cfb79bb3ac"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "coefs_now = wave_coeffs[1]['ddd'][0,:,:]\n",
    "plt.imshow(coefs_now, cmap = 'gray')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "zmLeUb9AXuaH",
    "outputId": "7f3e8783-22f0-4a4e-bd32-1b06a39ad3ea"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Haar + 2D: more interpretable\n",
    "\n",
    "It seems applying a Haar along spectral dimension and then 2d slice-wise is more interpretable. Let's just go for that."
   ],
   "metadata": {
    "id": "vU_1Ql57bF6Y"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "haar_dat = np.zeros_like(dat, dtype = float)\n",
    "for i in range(dat.shape[1]):\n",
    "  for j in range(dat.shape[2]):\n",
    "    cA, cD = pywt.dwt(dat[:, i, j], wavelet = 'haar', mode = 'periodization')\n",
    "    haar_dat[:, i, j] = np.concatenate((cA, cD))\n",
    "\n",
    "plt.plot(haar_dat[:, 0, 0])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "4ZXKX_CobQT6",
    "outputId": "2fa42145-fb1c-4a68-cb41-10094e62f5ab"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "wave_dat = list()\n",
    "\n",
    "for slice in range(haar_dat.shape[0]):\n",
    "  haar_slice = haar_dat[slice, :, :]\n",
    "  coeffs = pywt.wavedec2(haar_slice, \"db2\", level = 3, mode = \"periodization\")\n",
    "  wave_dat = wave_dat + [coeffs]"
   ],
   "metadata": {
    "id": "vpeMc3sFc4ZI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "cH1_pooled = np.array([wave_dat[slice][3][0] for slice in range(len(wave_dat))])\n",
    "cH1_pooled.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ki5MsqfhFEP",
    "outputId": "fc479538-3178-4364-9788-ba64f6984115"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_sparsified_array(original_array, prob_quantile):\n",
    "  norm_list = np.array([np.sum(original_array[:, i, j]**2) for i in range(original_array.shape[1]) for j in range(original_array.shape[2])])\n",
    "  quantile_threshold = np.quantile(norm_list, q = 1 - prob_quantile)\n",
    "  sparse_array = np.zeros_like(original_array, dtype = float)\n",
    "  for i in range(original_array.shape[1]):\n",
    "    for j in range(original_array.shape[2]):\n",
    "      if np.sum(original_array[:, i, j]**2)>quantile_threshold:\n",
    "        sparse_array[:, i, j] = original_array[:, i, j]\n",
    "  return sparse_array\n",
    "\n",
    "sparsified_array = get_sparsified_array(cH1_pooled, 0.1)"
   ],
   "metadata": {
    "id": "TW7qS37Flu1p"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We flatten each slice. We get a 2d matrix that needs to be transposed."
   ],
   "metadata": {
    "id": "5A5O11KowZOu"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "sparsified_array_flat = torch.flatten(torch.tensor(sparsified_array), 1)\n",
    "sparsified_array_flat = (np.transpose(sparsified_array_flat)).numpy()"
   ],
   "metadata": {
    "id": "vPVJBh1gv_8Z"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we check how many row sums are nonzero that is the block sparsity."
   ],
   "metadata": {
    "id": "eMgZMehQzy3k"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "block_sparsity = np.mean(np.sum(sparsified_array_flat**2, axis = 1) != 0.)\n",
    "block_sparsity"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v5BMog_ItBoS",
    "outputId": "cb81b713-dca5-40df-cac0-aff936757fab"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's initialize this as the signal matrix."
   ],
   "metadata": {
    "id": "_VA-DrY50KWr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "X_signal = sparsified_array_flat[:, :10]"
   ],
   "metadata": {
    "id": "6VsXcYQ20UUG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Older code"
   ],
   "metadata": {
    "id": "mjC6nK9TXwrB"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "haar_dat = np.zeros_like(padded_dat, dtype = float)\n",
    "\n",
    "for i in range(padded_dat.shape[0]):\n",
    "  for j in range(padded_dat.shape[1]):\n",
    "    padded_dat_slice = padded_dat[i,j,:]\n",
    "    cA_haar, cD_haar = pywt.dwt(padded_dat_slice, 'haar')\n",
    "    haar_dat[i, j, :] = np.concatenate((cA_haar, cD_haar))\n",
    "\n",
    "plt.imshow(haar_dat[:, :, 96])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 473
    },
    "id": "LftFCbaaQ_pF",
    "outputId": "ce5caaf8-395e-417f-ded1-2dad7c9926b9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "wave_dat = np.zeros_like(haar_dat, dtype = float)\n",
    "\n",
    "for slice in range(haar_dat.shape[2]):\n",
    "  haar_dat_slice = haar_dat[:, :, slice]\n",
    "  coeffs = pywt.wavedec2(haar_dat_slice, \"db1\", level = 3, mode = \"periodization\")\n",
    "  cA3, (cH3, cV3, cD3), (cH2, cV2, cD2), (cH1, cV1, cD1) = coeffs\n",
    "  wave_dat[:32, :32, slice] = cA3  # Upper left\n",
    "  wave_dat[32:64, :32, slice] = cH3  # Upper right\n",
    "  wave_dat[:32, 32:64, slice] = cV3  # Lower left\n",
    "  wave_dat[32:64, 32:64, slice] = cD3\n",
    "  wave_dat[64:128, :64, slice] = cH2\n",
    "  wave_dat[:64, 64:128, slice] = cV2\n",
    "  wave_dat[64:128, 64:128, slice] = cD2\n",
    "  wave_dat[128:256, :128, slice] = cH1\n",
    "  wave_dat[:128, 128:256, slice] = cV1\n",
    "  wave_dat[128:256, 128:256, slice] = cD1\n",
    "plt.imshow(wave_dat[:, :, 50], cmap = 'gray')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 473
    },
    "id": "FgIQjHYZWpsO",
    "outputId": "121819b9-b982-4e26-9341-9adc9c583772"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "wave_dat_approx_fiber = wave_dat[:32, :32, :]\n",
    "wave_dat_cH3_fiber = wave_dat[32:64, :32, :]\n",
    "wave_dat_cV3_fiber = wave_dat[:32, 32:64, :]\n",
    "wave_dat_cD3_fiber = wave_dat[32:64, 32:64, :]\n",
    "wave_dat_cH2_fiber = wave_dat[64:128, :64, :]\n",
    "wave_dat_cV2_fiber = wave_dat[:64, 64:128, :]\n",
    "wave_dat_cD2_fiber = wave_dat[64:128, 64:128, :]\n",
    "wave_dat_cH1_fiber = wave_dat[128:256, :128, :]\n",
    "wave_dat_cV1_fiber = wave_dat[:128, 128:256, :]\n",
    "wave_dat_cD1_fiber = wave_dat[128:256, 128:256, :]"
   ],
   "metadata": {
    "id": "GG34WbfoZzBF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "cD3.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nx8Jqa7qrfYe",
    "outputId": "0dc2789e-0e64-4dee-a439-e4c9c543f5a8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Comparison of norms"
   ],
   "metadata": {
    "id": "j1-o5BrrvC5O"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "wave_dat = torch.tensor(wave_dat)\n",
    "\n",
    "def get_norm(df, norm_order):\n",
    "  return torch.norm(torch.tensor(df), p = norm_order, dim = (0,1))\n",
    "\n",
    "# approximation\n",
    "wave_dat_approx_norm_1 = get_norm(wave_dat_cH2_fiber, 1)\n",
    "plt.plot(range(128, 256), wave_dat_approx_norm_1[128:256])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "id": "76HyzG5GvFyw",
    "outputId": "d67153a0-ad0f-4d09-bacb-a8f8a887307c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "haar_dat_norm_2[10], torch.norm(torch.flatten(haar_dat[:,:,10]), p=2)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WA1RSXoL3O3S",
    "outputId": "f5849c9a-bd83-40b5-9297-483c85eb745a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.plot(range(128, 256), haar_dat_norm_half[128:256])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "rhmWmfBe3Dvh",
    "outputId": "ea3025ab-921a-4bda-a3db-e2dfb92868fb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "x_test = 1.5**np.arange(20)\n",
    "pywt.dwt(x_test, 'haar')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ivvu3ft9Y49",
    "outputId": "eb8eace9-0375-481c-c82a-0a413d3c31db"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "x = haar_dat[0:2, 0:2, 0:3]\n",
    "print(x)\n",
    "# print(torch.norm(x[:, :, 0], p = 1))\n",
    "print(\"\\nNext\\n\")\n",
    "# z = torch.norm(x, p = 1, dim = (0,1))\n",
    "# print(z)\n",
    "z = torch.flatten(x, dim = (0, 1))\n",
    "print(z)"
   ],
   "metadata": {
    "id": "RNpED5khzxUo"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Approximate Message Passing"
   ],
   "metadata": {
    "id": "pmQNczTdU_j5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Oct 19 02:10:26 2023\n",
    "\n",
    "@author: apratimdey\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import Generator\n",
    "import cvxpy as cvx\n",
    "from pandas import DataFrame, concat\n",
    "import time\n",
    "# import amp_iteration as amp\n",
    "# from minimax_tau_threshold import minimax_tau_threshold\n",
    "\n",
    "# from EMS.manager import do_on_cluster, get_gbq_credentials, do_test_experiment, read_json, unroll_experiment\n",
    "# from dask.distributed import Client, LocalCluster\n",
    "# from dask_jobqueue import SLURMCluster\n",
    "# import dask\n",
    "# import coiled\n",
    "# import logging\n",
    "# import json\n",
    "\n",
    "# logging.basicConfig(level=logging.INFO)\n",
    "# log_gbq = logging.getLogger('pandas_gbq')\n",
    "# log_gbq.setLevel(logging.DEBUG)\n",
    "# log_gbq.addHandler(logging.StreamHandler())\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "# logging.getLogger('jax').setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "def seed(iter_count: int,\n",
    "         nonzero_rows: float,\n",
    "         num_measurements: float,\n",
    "         signal_nrow: float,\n",
    "         signal_ncol: float,\n",
    "         err_tol: float,\n",
    "         mc: float,\n",
    "         sparsity_tol: float) -> int:\n",
    "    return round(1 + round(iter_count*1000) + round(nonzero_rows * 1000) + round(num_measurements * 1000) + round(signal_nrow * 1000) + round(signal_ncol * 1000) + round(err_tol * 100000) + round(mc * 100000) + round(sparsity_tol * 1000000))\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def james_stein_nonsingular_vec(y,\n",
    "                                Sigma_inv):\n",
    "    d = len(y)\n",
    "    quad_whitening = jnp.dot(y, jnp.dot(Sigma_inv, y))\n",
    "    return jax.lax.cond(quad_whitening > (d-2),\n",
    "                    lambda y: y * (1 - ((d-2)/quad_whitening)),   # True branch (lambda function)\n",
    "                    lambda y: jnp.zeros(d),  # False branch (lambda function)\n",
    "                    y)  # Operand to pass to selected branch\n",
    "\n",
    "\n",
    "def james_stein_nonsingular(X: np.ndarray, Sigma_inv: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Applies james stein rowwise to denoise Y.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Y : np.ndarray\n",
    "        Noisy signal.\n",
    "    Sigma_inv : np.ndarray\n",
    "        Noise precision matrix.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \"\"\"\n",
    "    d = X.shape[1]\n",
    "    quad_whitening = np.sum(X * np.matmul(X, Sigma_inv), axis=1)\n",
    "    james_stein_coeff = np.where(quad_whitening > (d-2), 1 - ((d-2)/quad_whitening), 0.0)\n",
    "    return X * james_stein_coeff[:, np.newaxis]\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def james_stein_diagonal_vec(y,\n",
    "                             diag_inv):\n",
    "    d = len(y)\n",
    "    quad_whitening = jnp.sum(diag_inv * y**2)\n",
    "    return jax.lax.cond(quad_whitening > (d-2),\n",
    "                    lambda y: y * (1 - ((d-2)/quad_whitening)),   # True branch (lambda function)\n",
    "                    lambda y: jnp.zeros(d),  # False branch (lambda function)\n",
    "                    y)  # Operand to pass to selected branch\n",
    "\n",
    "\n",
    "def james_stein_diagonal(X, diag_inv):\n",
    "    d = X.shape[1]\n",
    "    quad_whitening = np.sum(X**2 * diag_inv, axis=1)\n",
    "    james_stein_coeff = np.where(quad_whitening > (d-2), 1 - ((d-2)/quad_whitening), 0.0)\n",
    "    return X * james_stein_coeff[:, np.newaxis]\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def james_stein_singular_vec(y,\n",
    "                             Sigma_eigvecs,\n",
    "                             nonzero_indices_int,\n",
    "                             zero_indices_int,\n",
    "                             Sigma_nonzero_eigvals_inv):\n",
    "\n",
    "    # changing coordinates to get uncorrelated components\n",
    "    y_indep = jnp.matmul(Sigma_eigvecs.T, y)\n",
    "\n",
    "    y_indep_nonzero =  y_indep[jnp.array(nonzero_indices_int)]\n",
    "\n",
    "    # apply denoiser on these coordinates to estimate the signal in the new basis on indep coordinates\n",
    "    signal_newbasis_indep = james_stein_diagonal_vec(y_indep_nonzero, Sigma_nonzero_eigvals_inv)\n",
    "\n",
    "    # when D has a 0 entry, it means we have perfect precision\n",
    "    # zero_indices = ~nonzero_indices\n",
    "    y_indep_zero = y_indep[jnp.array(zero_indices_int)]\n",
    "    signal_newbasis_zero = y_indep_zero\n",
    "\n",
    "    # combine the two to get signal_newbasis\n",
    "    signal_newbasis = jnp.concatenate((signal_newbasis_zero, signal_newbasis_indep))\n",
    "    # signal_newbasis = np.zeros(len(y), dtype = float)\n",
    "    # signal_newbasis[nonzero_indices] = signal_newbasis_indep\n",
    "    # signal_newbasis[zero_indices] = signal_newbasis_zero\n",
    "\n",
    "    # we have identified U.T @ signal, now we need to get signal i.e revert to original coordinates\n",
    "    signal_originalbasis = jnp.matmul(Sigma_eigvecs, signal_newbasis)\n",
    "\n",
    "    return signal_originalbasis\n",
    "\n",
    "\n",
    "\n",
    "def james_stein_singular(X, Sigma_eigvecs,\n",
    "                         nonzero_indices_int, zero_indices_int, Sigma_nonzero_eigvals_inv):\n",
    "    # changing coordinates to get uncorrelated components\n",
    "    X_indep = np.matmul(X, Sigma_eigvecs)\n",
    "\n",
    "    X_indep_nonzero =  X_indep[:, nonzero_indices_int]\n",
    "\n",
    "    # apply denoiser on these coordinates to estimate the signal in the new basis on indep coordinates\n",
    "    signal_newbasis_indep = james_stein_diagonal(X_indep_nonzero, Sigma_nonzero_eigvals_inv)\n",
    "\n",
    "    # when D has a 0 entry, it means we have perfect precision\n",
    "    # zero_indices = ~nonzero_indices\n",
    "    X_indep_zero = X_indep[:, zero_indices_int]\n",
    "    signal_newbasis_zero = X_indep_zero\n",
    "\n",
    "    # combine the two to get signal_newbasis\n",
    "    signal_newbasis = np.zeros(X.shape, dtype = float)\n",
    "    signal_newbasis[:, nonzero_indices_int] = signal_newbasis_indep\n",
    "    signal_newbasis[:, zero_indices_int] = signal_newbasis_zero\n",
    "\n",
    "    # we have identified U.T @ signal, now we need to get signal i.e revert to original coordinates\n",
    "    signal_originalbasis = np.matmul(signal_newbasis, Sigma_eigvecs.T)\n",
    "\n",
    "    return signal_originalbasis\n",
    "\n",
    "\n",
    "def update_signal_noisy(A: float,\n",
    "                        signal_denoised_prev: float,\n",
    "                        Residual_prev: float):\n",
    "    return signal_denoised_prev + np.matmul(A.T, Residual_prev)\n",
    "\n",
    "\n",
    "def update_signal_denoised_nonsingular(signal_noisy_current: float,\n",
    "                                       noise_cov_current_inv: float):\n",
    "    return james_stein_nonsingular(signal_noisy_current, noise_cov_current_inv)\n",
    "\n",
    "\n",
    "def update_signal_denoised_singular(signal_noisy_current: float,\n",
    "                                    noise_cov_current_eigvecs: float,\n",
    "                                    noise_cov_current_nonzero_indices_int,\n",
    "                                    noise_cov_current_zero_indices_int,\n",
    "                                    noise_cov_current_nonzero_eigvals_inv: float):\n",
    "    return james_stein_singular(signal_noisy_current, noise_cov_current_eigvecs,\n",
    "                                            noise_cov_current_nonzero_indices_int,\n",
    "                                            noise_cov_current_zero_indices_int,\n",
    "                                            noise_cov_current_nonzero_eigvals_inv)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def james_stein_onsager_nonsingular(X,\n",
    "                                    Z,\n",
    "                                    Sigma_inv):\n",
    "    X = jnp.array(X)\n",
    "    dd_jacobian = jax.jacfwd(james_stein_nonsingular_vec, argnums=0)\n",
    "    # dd_jacobian = jax.jit(jax.jacfwd(james_stein_nonsingular_vec, argnums=0))\n",
    "    jac_vectorized = jax.vmap(dd_jacobian, in_axes = (0, None))\n",
    "    sum_jacobians = jac_vectorized(X, Sigma_inv).sum(axis = 0)\n",
    "    onsager_term = jnp.matmul(Z, sum_jacobians.T)\n",
    "    return onsager_term / Z.shape[0]\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def james_stein_onsager_singular(X,\n",
    "                                 Z,\n",
    "                                 Sigma_eigvecs,\n",
    "                                 nonzero_indices_int,\n",
    "                                 zero_indices_int,\n",
    "                                 Sigma_nonzero_eigvals_inv):\n",
    "    X = jnp.array(X)\n",
    "    # selected_rows = rng.choice(X.shape[0], int(selected_rows_frac*X.shape[0]), replace = False)\n",
    "    dd_jacobian = jax.jacfwd(james_stein_singular_vec, argnums=0)\n",
    "    # dd_jacobian = jax.jit(jax.jacfwd(james_stein_singular_vec, argnums=0))\n",
    "    jac_vectorized = jax.vmap(dd_jacobian, in_axes = (0, None, None, None, None))\n",
    "    sum_jacobians = jac_vectorized(X, Sigma_eigvecs, nonzero_indices_int, zero_indices_int, Sigma_nonzero_eigvals_inv).sum(axis = 0)\n",
    "    onsager_term = jnp.matmul(Z, sum_jacobians.T)\n",
    "    return onsager_term / Z.shape[0]\n",
    "\n",
    "\n",
    "# def warm_up():\n",
    "#     y = np.ones(5, dtype = float)\n",
    "#     Sigma_inv = np.eye(5, dtype = float)\n",
    "#     res1 = james_stein_nonsingular_vec(y, Sigma_inv)\n",
    "#     res2 = james_stein_diagonal_vec(y, np.ones(5, dtype = float))\n",
    "#     res3 = james_stein_singular_vec(y, Sigma_inv, np.array([0,1,2]), np.array([3,4]), np.ones(3, dtype = float))\n",
    "#     res4 = james_stein_onsager_nonsingular(np.ones((1000,5), dtype = float), Sigma_inv, Sigma_inv, np.array([0]), 1.0)\n",
    "#     res5 = james_stein_onsager_singular(np.ones((1000,5), dtype = float), Sigma_inv, Sigma_inv, np.array([0,1,2]), np.array([3,4]), np.ones(3, dtype = float), np.array([0]), 1.0)\n",
    "#     return True\n",
    "\n",
    "\n",
    "def update_residual_singular(A: float,\n",
    "                             Y: float,\n",
    "                             signal_noisy_current: float,\n",
    "                             signal_denoised_current: float,\n",
    "                             Residual_prev: float,\n",
    "                             noise_cov_current_eigvecs: float,\n",
    "                             noise_cov_current_nonzero_indices_int,\n",
    "                             noise_cov_current_zero_indices_int,\n",
    "                             noise_cov_current_nonzero_eigvals_inv: float):\n",
    "    naive_residual = Y - np.matmul(A, signal_denoised_current)\n",
    "    onsager_term_ = james_stein_onsager_singular(signal_noisy_current,\n",
    "                                                         Residual_prev,\n",
    "                                                         noise_cov_current_eigvecs,\n",
    "                                                         noise_cov_current_nonzero_indices_int,\n",
    "                                                         noise_cov_current_zero_indices_int,\n",
    "                                                         noise_cov_current_nonzero_eigvals_inv)\n",
    "    return naive_residual + onsager_term_\n",
    "\n",
    "\n",
    "def update_residual_nonsingular(A: float,\n",
    "                                Y: float,\n",
    "                                signal_noisy_current: float,\n",
    "                                signal_denoised_current: float,\n",
    "                                Residual_prev: float,\n",
    "                                noise_cov_current_inv: float):\n",
    "    naive_residual = Y - np.matmul(A, signal_denoised_current)\n",
    "    onsager_term_ = james_stein_onsager_nonsingular(signal_noisy_current,\n",
    "                                                            Residual_prev,\n",
    "                                                            noise_cov_current_inv)\n",
    "    return naive_residual + onsager_term_\n",
    "\n",
    "\n",
    "def amp_iteration_nonsingular(A: float,\n",
    "                              Y: float,\n",
    "                              signal_denoised_prev: float,\n",
    "                              Residual_prev: float,\n",
    "                              noise_cov_current_inv: float):\n",
    "    signal_noisy_current = update_signal_noisy(A, signal_denoised_prev, Residual_prev)\n",
    "    # noise_cov_current = Residual_prev.T @ Residual_prev/A.shape[0]\n",
    "    signal_denoised_current = update_signal_denoised_nonsingular(signal_noisy_current, noise_cov_current_inv)\n",
    "    Residual_current = update_residual_nonsingular(A,\n",
    "                                                   Y,\n",
    "                                                   signal_noisy_current,\n",
    "                                                   signal_denoised_current,\n",
    "                                                   Residual_prev,\n",
    "                                                   noise_cov_current_inv)\n",
    "    return {'signal_denoised_current': signal_denoised_current,\n",
    "            'Residual_current': Residual_current}\n",
    "\n",
    "\n",
    "def amp_iteration_singular(A: float,\n",
    "                           Y: float,\n",
    "                           signal_denoised_prev: float,\n",
    "                           Residual_prev: float,\n",
    "                           noise_cov_current_eigvecs: float,\n",
    "                           noise_cov_current_nonzero_indices_int,\n",
    "                           noise_cov_current_zero_indices_int,\n",
    "                           noise_cov_current_nonzero_eigvals_inv: float):\n",
    "    signal_noisy_current = update_signal_noisy(A, signal_denoised_prev, Residual_prev)\n",
    "    # noise_cov_current = Residual_prev.T @ Residual_prev/A.shape[0]\n",
    "    signal_denoised_current = update_signal_denoised_singular(signal_noisy_current, noise_cov_current_eigvecs,\n",
    "                                                              noise_cov_current_nonzero_indices_int,\n",
    "                                                              noise_cov_current_zero_indices_int,\n",
    "                                                              noise_cov_current_nonzero_eigvals_inv)\n",
    "    Residual_current = update_residual_singular(A, Y, signal_noisy_current, signal_denoised_current, Residual_prev,\n",
    "                                                noise_cov_current_eigvecs, noise_cov_current_nonzero_indices_int,\n",
    "                                                noise_cov_current_zero_indices_int,\n",
    "                                                noise_cov_current_nonzero_eigvals_inv)\n",
    "    return {'signal_denoised_current': signal_denoised_current,\n",
    "            'Residual_current': Residual_current}\n",
    "\n",
    "\n",
    "# def warm_up_2():\n",
    "#     res1 = amp_iteration_nonsingular(np.eye(1000), np.eye(1000), 2*np.eye(1000), np.eye(1000), np.eye(1000), np.array([0]), 1.0)\n",
    "#     res2 = amp_iteration_singular(np.eye(3), np.eye(3), 2*np.eye(3), np.eye(3), np.eye(3), np.arange(2), np.array([2]), np.ones(2, dtype = float), np.array([0]), 1.0)\n",
    "#     return True\n",
    "\n",
    "\n",
    "def gen_iid_normal_mtx(num_measurements, signal_nrow, rng):\n",
    "    \"\"\"\n",
    "    Generates a single random num_measurements by signal_nrow matrix with iid signal_nrow(0,1) entries\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_measurements : int\n",
    "        Number of rows of measurement matrix.\n",
    "    signal_nrow : int\n",
    "        Number of rows of signal matrix.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        num_measurements by signal_nrow matrix.\n",
    "\n",
    "    \"\"\"\n",
    "    return rng.normal(0, 1, (num_measurements, signal_nrow))\n",
    "\n",
    "\n",
    "def recovery_stats(X_true: float,\n",
    "              X_rec: float,\n",
    "              sparsity_tol: float,\n",
    "              A: np.ndarray,\n",
    "              Y_true: np.ndarray):\n",
    "\n",
    "    N, B = X_true.shape\n",
    "    n = Y_true.shape[0]\n",
    "    Y_rec = np.matmul(A, X_rec)\n",
    "\n",
    "    zero_indices_true = (np.apply_along_axis(np.linalg.norm, 1, X_true)==0)\n",
    "    zero_indices_rec = (np.apply_along_axis(np.linalg.norm, 1, X_rec)/np.sqrt(B)<=sparsity_tol)\n",
    "\n",
    "    nonzero_indices_true = (np.apply_along_axis(np.linalg.norm, 1, X_true)!=0)\n",
    "    nonzero_indices_rec = (np.apply_along_axis(np.linalg.norm, 1, X_rec)/np.sqrt(B)>sparsity_tol)\n",
    "\n",
    "    dict_observables = {\n",
    "                'rel_err': cvx.norm(X_true-X_rec, \"fro\").value/cvx.norm(X_true, \"fro\").value,\n",
    "                'rel_err_measurements': cvx.norm(Y_true-Y_rec, \"fro\").value/cvx.norm(Y_true, \"fro\").value,\n",
    "                'avg_err': cvx.norm(X_true - X_rec, \"fro\").value/np.sqrt(N*B),\n",
    "                'avg_err_measurements': cvx.norm(Y_true - Y_rec, \"fro\").value/np.sqrt(n*B),\n",
    "                'max_row_err': cvx.mixed_norm(X_true - X_rec, 2, np.inf).value/np.sqrt(B),\n",
    "                'max_row_err_measurements': cvx.mixed_norm(Y_true - Y_rec, 2, np.inf).value/np.sqrt(B),\n",
    "                'norm_2_1_true': cvx.mixed_norm(X_true, 2, 1).value/(N*np.sqrt(B)),\n",
    "                'norm_2_1_rec': cvx.mixed_norm(X_rec, 2, 1).value/(N*np.sqrt(B)),\n",
    "                'norm_2_2_true': cvx.mixed_norm(X_true, 2, 2).value/np.sqrt(N*B),\n",
    "                'norm_2_2_rec': cvx.mixed_norm(X_rec, 2, 2).value/np.sqrt(N*B),\n",
    "                'norm_2_infty_true': cvx.mixed_norm(X_true, 2, np.inf).value/np.sqrt(B),\n",
    "                'norm_2_infty_rec': cvx.mixed_norm(X_rec, 2, np.inf).value/np.sqrt(B),\n",
    "                'soft_sparsity': np.mean(np.apply_along_axis(np.linalg.norm, 1, X_rec)/np.sqrt(B) > sparsity_tol),\n",
    "                'nonzero_rows_rec': np.sum(np.apply_along_axis(np.linalg.norm, 1, X_rec)/np.sqrt(B) > sparsity_tol),\n",
    "                'tpr': sum(zero_indices_true * zero_indices_rec)/max(1, sum(zero_indices_true)),\n",
    "                'tnr': sum(nonzero_indices_true * nonzero_indices_rec)/max(1, sum(nonzero_indices_true)),\n",
    "                'norm_2_1_true_measurements': cvx.mixed_norm(Y_true, 2, 1).value/(n*np.sqrt(B)),\n",
    "                'norm_2_1_rec_measurements': cvx.mixed_norm(Y_rec, 2, 1).value/(n*np.sqrt(B)),\n",
    "                'norm_2_2_true_measurements': cvx.mixed_norm(Y_true, 2, 2).value/np.sqrt(n*B),\n",
    "                'norm_2_2_rec_measurements': cvx.mixed_norm(Y_rec, 2, 2).value/np.sqrt(n*B),\n",
    "                'norm_2_infty_true_measurements': cvx.mixed_norm(Y_true, 2, np.inf).value/np.sqrt(B),\n",
    "                'norm_2_infty_rec_measurements': cvx.mixed_norm(Y_rec, 2, np.inf).value/np.sqrt(B)\n",
    "                }\n",
    "\n",
    "    return dict_observables\n",
    "\n",
    "\n",
    "def add_row_to_df(dict_to_add, df):\n",
    "    return concat([df, DataFrame(dict_to_add, index = [0])], ignore_index=True)\n",
    "\n",
    "\n",
    "def run_amp_instance(**dict_params):\n",
    "\n",
    "    k = dict_params['nonzero_rows']\n",
    "    n = dict_params['num_measurements']\n",
    "    N = dict_params['signal_nrow']\n",
    "    B = dict_params['signal_ncol']\n",
    "    err_tol = dict_params['err_tol']\n",
    "    mc = dict_params['mc']\n",
    "    sparsity_tol = dict_params['sparsity_tol']\n",
    "    max_iter = dict_params['max_iter']\n",
    "    err_explosion_tol = dict_params['err_explosion_tol']\n",
    "\n",
    "    iter_count = 0\n",
    "\n",
    "    rng = np.random.default_rng(seed=seed(iter_count, k, n, N, B, err_tol, mc, sparsity_tol))\n",
    "    signal_true = np.zeros((N, B), dtype=float)\n",
    "    nonzero_indices = rng.choice(range(N), k, replace=False)\n",
    "    # signal_true[nonzero_indices, :] = rng.normal(0, 1, (k, B))\n",
    "    # signal_true[nonzero_indices, :] = rng.poisson(2, (k, B))\n",
    "    signal_true[nonzero_indices, :] = rng.binomial(1, 0.5, (k, B))\n",
    "    signal_true = np.array(signal_true)\n",
    "\n",
    "    A = gen_iid_normal_mtx(n, N, rng)/np.sqrt(n)\n",
    "    Y_true = np.matmul(A, signal_true)\n",
    "\n",
    "    sparsity = k/N\n",
    "    dict_params['sparsity'] = sparsity\n",
    "    dict_params['undersampling_ratio'] = n/N\n",
    "\n",
    "    output_df = None\n",
    "\n",
    "    iter_count = 0\n",
    "\n",
    "    signal_denoised_current = np.zeros((N, B), dtype = float)\n",
    "    Residual_current = Y_true\n",
    "\n",
    "    dict_observables = recovery_stats(signal_true,\n",
    "                               signal_denoised_current,\n",
    "                               sparsity_tol,\n",
    "                               A,\n",
    "                               Y_true)\n",
    "    rel_err = dict_observables['rel_err']\n",
    "    # rec_stats_dict['iter_count'] = iter_count\n",
    "    min_rel_err = rel_err\n",
    "\n",
    "    while iter_count<max_iter and rel_err>err_tol and rel_err<err_explosion_tol:\n",
    "        tick = time.perf_counter()\n",
    "\n",
    "        iter_count = iter_count + 1\n",
    "\n",
    "        # signal_denoised_prev = signal_denoised_current\n",
    "        # signal_denoised_current = None\n",
    "        # Residual_prev = Residual_current\n",
    "        # Residual_current = None\n",
    "        # noise_cov_current = np.matmul(Residual_prev.T, Residual_prev)/n\n",
    "        noise_cov_current = np.cov(Residual_current.T)\n",
    "        # noise_cov_current_diag = np.diag(np.var(Residual_prev, axis = 0))\n",
    "\n",
    "        D, U = np.linalg.eigh(noise_cov_current)\n",
    "        D = np.round(D, 10)\n",
    "\n",
    "        if np.all(D > 0):\n",
    "            noise_cov_current_inv = np.matmul(U * 1.0/D, U.T)\n",
    "            dict_current = amp_iteration_nonsingular(A, Y_true,\n",
    "                                                     signal_denoised_current,\n",
    "                                                     Residual_current, noise_cov_current_inv)\n",
    "        else:\n",
    "            nonzero_indices = (D > 0)\n",
    "            nonzero_indices_int = np.where(nonzero_indices)[0]\n",
    "            zero_indices_int = np.where(~nonzero_indices)[0]\n",
    "            D_nonzero_inv = 1/D[nonzero_indices_int]\n",
    "            dict_current = amp_iteration_singular(A, Y_true,\n",
    "                                                  signal_denoised_current,\n",
    "                                                  Residual_current,\n",
    "                                                  U, nonzero_indices_int, zero_indices_int, D_nonzero_inv)\n",
    "\n",
    "        signal_denoised_current = dict_current['signal_denoised_current']\n",
    "        Residual_current = dict_current['Residual_current']\n",
    "        dict_observables = recovery_stats(signal_true,\n",
    "                                   signal_denoised_current,\n",
    "                                   sparsity_tol,\n",
    "                                   A,\n",
    "                                   Y_true)\n",
    "        rel_err = dict_observables['rel_err']\n",
    "        min_rel_err = min(rel_err, min_rel_err)\n",
    "        tock = time.perf_counter() - tick\n",
    "        if iter_count % 50 == 0:\n",
    "            dict_observables['avg_trace_resid_cov'] = np.mean(D)\n",
    "            dict_observables['min_rel_err'] = min_rel_err\n",
    "            dict_observables['iter_count'] = iter_count\n",
    "            dict_observables['time_seconds'] = round(tock, 2)\n",
    "            combined_dict = {**dict_params, **dict_observables}\n",
    "            output_df = add_row_to_df(combined_dict, output_df)\n",
    "\n",
    "    if iter_count % 50 != 0:\n",
    "        dict_observables['avg_trace_resid_cov'] = np.mean(D)\n",
    "        dict_observables['min_rel_err'] = min_rel_err\n",
    "        dict_observables['iter_count'] = iter_count\n",
    "        dict_observables['time_seconds'] = round(tock, 2)\n",
    "        combined_dict = {**dict_params, **dict_observables}\n",
    "        output_df = add_row_to_df(combined_dict, output_df)\n",
    "\n",
    "    #return DataFrame(data = {**dict_params, **dict_observables}).set_index('iter_count')\n",
    "    return output_df\n",
    "\n",
    "\n",
    "def test_experiment() -> dict:\n",
    "    exp = {'table_name':'amp-test',\n",
    "           'params': [{\n",
    "               'nonzero_rows': [30],\n",
    "               'num_measurements': [320],\n",
    "               'signal_nrow': [1000],\n",
    "               'signal_ncol': [5],\n",
    "               'max_iter': [1],\n",
    "               'err_tol': [1e-5],\n",
    "               'sparsity_tol': [1e-4],\n",
    "               'err_explosion_tol': [100],\n",
    "               'mc': [0],\n",
    "               'selected_rows_frac': [1.0]\n",
    "                }]\n",
    "           }\n",
    "    # exp = dict(table_name='amp-integer-grids',\n",
    "    #             base_index=0,\n",
    "    #             db_url='sqlite:///data/EMS.db3',\n",
    "    #             multi_res=[{\n",
    "    #                 'nonzero_rows': list(range(1, 100)),\n",
    "    #                 'num_measurements': list(range(1, 100)),\n",
    "    #                 'signal_nrow': [100],\n",
    "    #                 'signal_ncol': [1, 2, 3, 4, 5],\n",
    "    #                 'mc': list(range(100)),\n",
    "    #                 'err_tol': [1e-5],\n",
    "    #                 'sparsity_tol': [1e-4]\n",
    "    #             }])\n",
    "    # exp = dict(table_name='comr-N50-larger-grids',\n",
    "    #             base_index=0,\n",
    "    #             db_url='sqlite:///data/EMS.db3',\n",
    "    #             multi_res=[{\n",
    "    #                 'nonzero_rows': list(range(1, 50)),\n",
    "    #                 'num_measurements': list(range(1, 50)),\n",
    "    #                 'signal_nrow': [50],\n",
    "    #                 'signal_ncol': [1, 2, 3, 4, 5],\n",
    "    #                 'mc': list(range(100)),\n",
    "    #                 'err_tol': [1e-5],\n",
    "    #                 'sparsity_tol': [1e-4]\n",
    "    #             }])\n",
    "    return exp\n",
    "\n",
    "def do_sherlock_experiment(json_file: str):\n",
    "    exp = read_json(json_file)\n",
    "    nodes = 1000\n",
    "    with SLURMCluster(queue='normal,owners,donoho,hns,stat',\n",
    "                      cores=1, memory='4GiB', processes=1,\n",
    "                      walltime='24:00:00') as cluster:\n",
    "        cluster.scale(jobs=nodes)\n",
    "        logging.info(cluster.job_script())\n",
    "        with Client(cluster) as client:\n",
    "            do_on_cluster(exp, run_amp_instance, client, credentials=get_gbq_credentials())\n",
    "        cluster.scale(0)\n",
    "\n",
    "\n",
    "def do_coiled_experiment(json_file: str):\n",
    "    exp = read_json(json_file)\n",
    "    # logging.info(f'{json.dumps(dask.config.config, indent=4)}')\n",
    "    software_environment = 'adonoho/amp_matrix_recovery'\n",
    "    # coiled.delete_software_environment(name=software_environment)\n",
    "    logging.info('Creating environment.')\n",
    "    coiled.create_software_environment(\n",
    "        name=software_environment,\n",
    "        conda=\"environment-coiled.yml\",\n",
    "        pip=[\n",
    "            \"git+https://GIT_TOKEN@github.com/adonoho/EMS.git\"\n",
    "        ]\n",
    "    )\n",
    "    with coiled.Cluster(software=software_environment,\n",
    "                        n_workers=960, worker_vm_types=['n1-standard-1'],\n",
    "                        use_best_zone=True, spot_policy='spot') as cluster:\n",
    "        with Client(cluster) as client:\n",
    "            do_on_cluster(exp, run_amp_instance, client, credentials=get_gbq_credentials())\n",
    "\n",
    "\n",
    "def do_local_experiment():\n",
    "    exp = test_experiment()\n",
    "    logging.info(f'{json.dumps(dask.config.config, indent=4)}')\n",
    "    with LocalCluster(dashboard_address='localhost:8787') as cluster:\n",
    "        with Client(cluster) as client:\n",
    "            # do_on_cluster(exp, run_amp_instance, client, credentials=None)\n",
    "            do_on_cluster(exp, run_amp_instance, client, credentials=get_gbq_credentials())\n",
    "\n",
    "\n",
    "def read_and_do_local_experiment(json_file: str):\n",
    "    exp = read_json(json_file)\n",
    "    with LocalCluster(dashboard_address='localhost:8787', n_workers=32) as cluster:\n",
    "        with Client(cluster) as client:\n",
    "            # do_on_cluster(exp, run_amp_instance, client, credentials=None)\n",
    "            do_on_cluster(exp, run_amp_instance, client, credentials=get_gbq_credentials())\n",
    "\n",
    "\n",
    "def do_test_exp():\n",
    "    exp = test_experiment()\n",
    "    with LocalCluster(dashboard_address='localhost:8787') as cluster:\n",
    "        with Client(cluster) as client:\n",
    "            do_test_experiment(exp, run_amp_instance, client, credentials=get_gbq_credentials())\n",
    "\n",
    "\n",
    "def do_test():\n",
    "    exp = test_experiment()\n",
    "    print(exp)\n",
    "    pass\n",
    "    df = run_amp_instance(dict_params = exp)\n",
    "    df.to_csv(\"temp.csv\")\n",
    "\n",
    "\n",
    "def count_params(json_file: str):\n",
    "    exp = read_json(json_file)\n",
    "    params = unroll_experiment(exp)\n",
    "    logging.info(f'Count of instances: {len(params)}.')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "Phdg4G94glOD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def run_amp_instance(X_signal,\n",
    "                     nonzero_rows,\n",
    "                     num_measurements,\n",
    "                     err_tol,\n",
    "                     mc,\n",
    "                     sparsity_tol,\n",
    "                     max_iter,\n",
    "                     err_explosion_tol):\n",
    "\n",
    "    k = nonzero_rows\n",
    "    n = num_measurements\n",
    "    N = X_signal.shape[0]\n",
    "    B = X_signal.shape[1]\n",
    "    dict_params = {'nonzero_rows': k, 'num_measurements': n, 'signal_nrow': N, 'signal_ncol': B, 'mc': mc}\n",
    "\n",
    "    iter_count = 0\n",
    "\n",
    "    rng = np.random.default_rng(seed=seed(iter_count, k, n, N, B, err_tol, mc, sparsity_tol))\n",
    "    signal_true = X_signal\n",
    "\n",
    "    A = gen_iid_normal_mtx(n, N, rng)/np.sqrt(n)\n",
    "    Y_true = np.matmul(A, signal_true)\n",
    "\n",
    "    sparsity = k/N\n",
    "    dict_params['sparsity'] = sparsity\n",
    "    dict_params['undersampling_ratio'] = n/N\n",
    "\n",
    "    output_df = None\n",
    "\n",
    "    iter_count = 0\n",
    "\n",
    "    signal_denoised_current = np.zeros((N, B), dtype = float)\n",
    "    Residual_current = Y_true\n",
    "\n",
    "    dict_observables = recovery_stats(signal_true,\n",
    "                               signal_denoised_current,\n",
    "                               sparsity_tol,\n",
    "                               A,\n",
    "                               Y_true)\n",
    "    rel_err = dict_observables['rel_err']\n",
    "    # rec_stats_dict['iter_count'] = iter_count\n",
    "    min_rel_err = rel_err\n",
    "\n",
    "    while iter_count<max_iter and rel_err>err_tol and rel_err<err_explosion_tol:\n",
    "        tick = time.perf_counter()\n",
    "\n",
    "        iter_count = iter_count + 1\n",
    "\n",
    "        # signal_denoised_prev = signal_denoised_current\n",
    "        # signal_denoised_current = None\n",
    "        # Residual_prev = Residual_current\n",
    "        # Residual_current = None\n",
    "        # noise_cov_current = np.matmul(Residual_prev.T, Residual_prev)/n\n",
    "        noise_cov_current = np.cov(Residual_current.T)\n",
    "        # noise_cov_current_diag = np.diag(np.var(Residual_prev, axis = 0))\n",
    "\n",
    "        D, U = np.linalg.eigh(noise_cov_current)\n",
    "        D = np.round(D, 10)\n",
    "\n",
    "        if np.all(D > 0):\n",
    "            noise_cov_current_inv = np.matmul(U * 1.0/D, U.T)\n",
    "            dict_current = amp_iteration_nonsingular(A, Y_true,\n",
    "                                                     signal_denoised_current,\n",
    "                                                     Residual_current, noise_cov_current_inv)\n",
    "        else:\n",
    "            nonzero_indices = (D > 0)\n",
    "            nonzero_indices_int = np.where(nonzero_indices)[0]\n",
    "            zero_indices_int = np.where(~nonzero_indices)[0]\n",
    "            D_nonzero_inv = 1/D[nonzero_indices_int]\n",
    "            dict_current = amp_iteration_singular(A, Y_true,\n",
    "                                                  signal_denoised_current,\n",
    "                                                  Residual_current,\n",
    "                                                  U, nonzero_indices_int, zero_indices_int, D_nonzero_inv)\n",
    "\n",
    "        signal_denoised_current = dict_current['signal_denoised_current']\n",
    "        Residual_current = dict_current['Residual_current']\n",
    "        dict_observables = recovery_stats(signal_true,\n",
    "                                   signal_denoised_current,\n",
    "                                   sparsity_tol,\n",
    "                                   A,\n",
    "                                   Y_true)\n",
    "        rel_err = dict_observables['rel_err']\n",
    "        min_rel_err = min(rel_err, min_rel_err)\n",
    "        tock = time.perf_counter() - tick\n",
    "        print(\"iter count: \" + str(iter_count))\n",
    "        if iter_count % 50 == 0:\n",
    "            dict_observables['avg_trace_resid_cov'] = np.mean(D)\n",
    "            dict_observables['min_rel_err'] = min_rel_err\n",
    "            dict_observables['iter_count'] = iter_count\n",
    "            dict_observables['time_seconds'] = round(tock, 2)\n",
    "            combined_dict = {**dict_params, **dict_observables}\n",
    "            output_df = add_row_to_df(combined_dict, output_df)\n",
    "\n",
    "    if iter_count % 50 != 0:\n",
    "        dict_observables['avg_trace_resid_cov'] = np.mean(D)\n",
    "        dict_observables['min_rel_err'] = min_rel_err\n",
    "        dict_observables['iter_count'] = iter_count\n",
    "        dict_observables['time_seconds'] = round(tock, 2)\n",
    "        combined_dict = {**dict_params, **dict_observables}\n",
    "        output_df = add_row_to_df(combined_dict, output_df)\n",
    "\n",
    "    #return DataFrame(data = {**dict_params, **dict_observables}).set_index('iter_count')\n",
    "    return output_df\n"
   ],
   "metadata": {
    "id": "Ho4PdJughw3r"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df = run_amp_instance(X_signal,\n",
    "                 np.sum(np.sum(X_signal**2, axis = 1)!=0.),\n",
    "                 1170,\n",
    "                 1e-4,\n",
    "                 0,\n",
    "                 1e-4,\n",
    "                 1000,\n",
    "                 100)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "reehdHu32BYk",
    "outputId": "91e7c4fa-24c9-40c0-9913-a68869b455bc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df.to_csv(\"temp.csv\")"
   ],
   "metadata": {
    "id": "uUZvNVL646gS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tracking similarities across slices"
   ],
   "metadata": {
    "id": "C-pZP_e5MmJh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dat\n",
    "dat_transposed = np.transpose(dat, (2, 0, 1))\n",
    "dat_transposed.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r-5zTrUbMo64",
    "outputId": "b0b240d7-91ef-4247-87f3-160ebc8a742a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(dat[:, :, 0])\n",
    "print(dat_transposed[0, :, :])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fiZzB1_nRX3B",
    "outputId": "3dd96f6d-6982-4463-ddb2-efa37deda52b"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We create a data matrix whose each column is each flattened slice/face. After that we apply PCA."
   ],
   "metadata": {
    "id": "azqlqbvDTp7V"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "X_samples = np.zeros((dat_transposed.shape[1]*dat_transposed.shape[2], dat_transposed.shape[0]), dtype = float)\n",
    "for i in range(dat_transposed.shape[0]):\n",
    "    X_samples[:, i] = dat_transposed[i, :, :].flatten()\n"
   ],
   "metadata": {
    "id": "jFD7bXBAMrJZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_samples = scaler.fit_transform(X_samples)\n",
    "pca = PCA()  # for example, reducing to 2 components\n",
    "pca.fit(X_samples)  # 'data' is your numpy array"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 95
    },
    "id": "n0ABlmM4Pz2t",
    "outputId": "d3c17bc3-f386-440b-ad07-0d0b5b4b163f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "np.mean(X_samples[:, 0]), np.std(X_samples[:, 10]) # checking standardization"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DhKD-kAyXRB2",
    "outputId": "c1d8b0da-dc18-45f2-9341-ac8e5194fd6d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "transformed_data = pca.transform(X_samples)"
   ],
   "metadata": {
    "id": "YoReaB0NP3X_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.plot(pca.singular_values_)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "r7XoyoapP_AP",
    "outputId": "c016106a-eecb-4392-e81a-49348f7f482d"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install screenot"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C0NSVEK9WB_U",
    "outputId": "5f025bf6-8ebe-4934-b989-331b81131f65"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from screenot.ScreeNOT import adaptiveHardThresholding\n",
    "adaptiveHardThresholding(X_samples, )"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kPhMH-46gy5A",
    "outputId": "9c53e02e-e0d4-448e-f5a5-b81d381dd6db"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Verifying something about commutativity of wavelets"
   ],
   "metadata": {
    "id": "zUcQCtZZlls5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "x = np.random.normal(0, 1, (4, 4))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gfkcyX-RhKfQ",
    "outputId": "9e80815b-ffcc-4ae7-b58e-44373a0807c3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "wav2 = pywt.dwt2(x, 'db2', mode = 'periodization')\n",
    "wav2"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2twvcWHBltS3",
    "outputId": "7989ad11-1ec3-42b9-8d0f-31812579b5d9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "wav1_rows = np.zeros((4, 4), dtype = float)\n",
    "for row in range(x.shape[0]):\n",
    "  cA, cD = pywt.dwt(x[row, :], 'db2', mode = 'periodization')\n",
    "  wav1_rows[row, :] = np.concatenate((cA, cD))\n",
    "\n",
    "wav1_rows"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VcnFL6C1lzmn",
    "outputId": "bc104344-5e10-4131-974e-e5d0a3da5e8b"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "wav1_cols = np.zeros((4, 4), dtype = float)\n",
    "for col in range(wav1_rows.shape[1]):\n",
    "  cA, cD = pywt.dwt(wav1_rows[:, col], 'db2', mode = 'periodization')\n",
    "  wav1_cols[:, col] = np.concatenate((cA, cD))\n",
    "\n",
    "plt.imshow(wav1_cols)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AMdJ3sZxmL5Z",
    "outputId": "e73f8178-cc9f-49e5-fb26-f773a160594e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.imshow(wav1_cols)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 473
    },
    "id": "Fj7uAjrxsyfQ",
    "outputId": "b846c606-6aa9-4a68-b199-16d203a8e9a5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.imshow(x)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 473
    },
    "id": "I1wzV6nks0Pp",
    "outputId": "61244739-0e83-432f-a556-a6bf89aa80e3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "y = np.zeros(256**8)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "id": "sBiUzv7ps8jm",
    "outputId": "6d8309b6-6651-45f5-ac92-12e07497143a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "LEllPLCHuJUJ"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
